{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":191501,"sourceType":"datasetVersion","datasetId":82373},{"sourceId":11574562,"sourceType":"datasetVersion","datasetId":7256858},{"sourceId":11597836,"sourceType":"datasetVersion","datasetId":7273256},{"sourceId":11601577,"sourceType":"datasetVersion","datasetId":7276151}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ItalianSignNet study","metadata":{}},{"cell_type":"markdown","source":"### Importing Required Libraries","metadata":{"_kg_hide-output":true}},{"cell_type":"code","source":"!pip cache purge\n!pip install -q tensorflow-model-optimization\n!pip install tf-keras\n!pip install lime scikit-image","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_model_optimization as tfmot\n#import tf_keras as keras\nimport tensorflow.keras as keras\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score\n\nnp.random.seed(42)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Boiterplate","metadata":{}},{"cell_type":"code","source":"data_dir = '/kaggle/input/dataset-2025-04-23-eps-100-changed/dataset_20250423_200322_eps_100_changed/'\ntrain_path = f'{data_dir}/train'\ntest_path = f'{data_dir}/test'\nval_set = f'{data_dir}/validation'\n\n# Resizing the images to 30x30x3\nIMG_HEIGHT = 30\nIMG_WIDTH = 30\nchannels = 3\n\nNUM_CATEGORIES = len(os.listdir(train_path))\nlr = 0.001\nepochs = 30\n\nclasses = { 0:'Speed limit (20km/h)',\n            1:'Speed limit (30km/h)', \n            2:'Speed limit (50km/h)', \n            3:'Speed limit (60km/h)', \n            4:'Speed limit (70km/h)', \n            5:'Speed limit (80km/h)', \n            6:'End of speed limit (80km/h)', \n            7:'Speed limit (100km/h)', \n            8:'Speed limit (120km/h)', \n            9:'No passing', \n            10:'No passing veh over 3.5 tons', \n            11:'Right-of-way at intersection', \n            12:'Priority road', \n            13:'Yield', \n            14:'Stop', \n            15:'No vehicles', \n            16:'Veh > 3.5 tons prohibited', \n            17:'No entry', \n            18:'General caution', \n            19:'Dangerous curve left', \n            20:'Dangerous curve right', \n            21:'Double curve', \n            22:'Bumpy road', \n            23:'Slippery road', \n            24:'Road narrows on the right', \n            25:'Road work', \n            26:'Traffic signals', \n            27:'Pedestrians', \n            28:'Children crossing', \n            29:'Bicycles crossing', \n            30:'Beware of ice/snow',\n            31:'Wild animals crossing', \n            32:'End speed + passing limits', \n            33:'Turn right ahead', \n            34:'Turn left ahead', \n            35:'Ahead only', \n            36:'Go straight or right', \n            37:'Go straight or left', \n            38:'Keep right', \n            39:'Keep left', \n            40:'Roundabout mandatory', \n            41:'End of no passing', \n            42:'End no passing veh > 3.5 tons' }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing The Dataset","metadata":{}},{"cell_type":"code","source":"folders = os.listdir(train_path)\n\ntrain_number = []\nclass_num = []\n\nfor folder in folders:\n    train_files = os.listdir(train_path + '/' + folder)\n    train_number.append(len(train_files))\n    class_num.append(classes[int(folder)])\n    \n# Sorting the dataset on the basis of number of images in each class\nzipped_lists = zip(train_number, class_num)\nsorted_pairs = sorted(zipped_lists)\ntuples = zip(*sorted_pairs)\ntrain_number, class_num = [ list(tuple) for tuple in  tuples]\n\n# Plotting the number of images in each class\nplt.figure(figsize=(10,5))  \nplt.bar(class_num, train_number)\nplt.xticks(class_num, rotation='vertical')\nplt.show()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Collecting the Training Data","metadata":{}},{"cell_type":"code","source":"image_data = []\nimage_labels = []\n\nfor i in range(NUM_CATEGORIES):\n    path = data_dir + '/train/' + f\"{i:02d}\"\n\n    # Skip folder of label that not exists \n    if not os.path.isdir(path):\n        continue\n    images = os.listdir(path)\n\n    for img in images:\n        try:\n            image = cv2.imread(path + '/' + img)\n            image_fromarray = Image.fromarray(image, 'RGB')\n            resize_image = image_fromarray.resize((IMG_HEIGHT, IMG_WIDTH))\n            image_data.append(np.array(resize_image))\n            image_labels.append(f\"{i:02d}\")\n        except:\n            print(\"Error in \" + img)\n\n# Changing the list to numpy array\nimage_data = np.array(image_data)\nimage_labels = np.array(image_labels)\n\nprint(image_data.shape, image_labels.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Shuffling the training data","metadata":{}},{"cell_type":"code","source":"shuffle_indexes = np.arange(image_data.shape[0])\nnp.random.shuffle(shuffle_indexes)\nimage_data = image_data[shuffle_indexes]\nimage_labels = image_labels[shuffle_indexes]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Collecting validation set","metadata":{}},{"cell_type":"code","source":"X_train = image_data.astype('float32') / 255.\n\nval_data = []\nval_labels = []\nfor label_name in os.listdir(val_set):\n    label_dir = os.path.join(val_set, label_name)\n    if not os.path.isdir(label_dir):\n        continue\n    for img_file in os.listdir(label_dir):\n        img_path = os.path.join(label_dir, img_file)\n        img = cv2.imread(img_path)\n        img = Image.fromarray(img, 'RGB').resize((IMG_HEIGHT, IMG_WIDTH))\n        val_data.append(np.array(img))\n        val_labels.append(int(label_name))\n\nX_val = np.array(val_data, dtype='float32') / 255.\n\nprint(\"X_train.shape\", X_train.shape)\nprint(\"X_val.shape\",   X_val.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## One hot encoding the labels","metadata":{}},{"cell_type":"code","source":"y_train = keras.utils.to_categorical(image_labels, NUM_CATEGORIES)\nval_labels = np.array(val_labels) \ny_val = keras.utils.to_categorical(val_labels, NUM_CATEGORIES)\nprint(\"y_train.shape\", y_train.shape)\nprint(\"y_val.shape\",   y_val.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Making the model","metadata":{}},{"cell_type":"code","source":"model = keras.models.Sequential([ \n    keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu', input_shape=(IMG_HEIGHT,IMG_WIDTH,channels)),\n    keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'),\n    keras.layers.MaxPool2D(pool_size=(2, 2)),\n    keras.layers.BatchNormalization(axis=-1),\n    \n    keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n    keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu'),\n    keras.layers.MaxPool2D(pool_size=(2, 2)),\n    keras.layers.BatchNormalization(axis=-1),\n    \n    keras.layers.Flatten(),\n    keras.layers.Dense(512, activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(rate=0.5),\n    \n    keras.layers.Dense(43, activation='softmax')\n])\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def improved_model(input_shape=(IMG_HEIGHT,IMG_WIDTH,channels), num_classes=43):\n    inputs = keras.Input(shape=input_shape)\n    x = keras.layers.SeparableConv2D(32, (3,3), activation='relu', padding='same')(inputs)\n    x = keras.layers.BatchNormalization()(x)\n    # Blocco residual 1\n    res = x\n    x = keras.layers.SeparableConv2D(32,(3,3),activation='relu',padding='same')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Add()([x, res])\n    x = keras.layers.MaxPool2D()(x)\n\n    # Blocco residual 2\n    res = x\n    x = keras.layers.SeparableConv2D(32,(3,3),activation='relu',padding='same')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.SeparableConv2D(32,(3,3),activation='relu',padding='same')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Add()([x, res])\n    x = keras.layers.MaxPool2D()(x)\n\n    x = keras.layers.GlobalAveragePooling2D()(x)\n    x = keras.layers.Dense(128, activation='relu')(x)\n    x = keras.layers.Dropout(0.5)(x)\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    model = keras.Model(inputs, outputs, name=\"improved_cnn\")\n    return model\n\nmodel_imp = improved_model()\nmodel_imp.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_slim = keras.models.Sequential([\n    keras.layers.Conv2D(16,(3,3),activation='relu',input_shape=(IMG_HEIGHT,IMG_WIDTH,channels),padding='same'),\n    keras.layers.MaxPool2D(),\n    keras.layers.Conv2D(32,(3,3),activation='relu',padding='same'),\n    keras.layers.MaxPool2D(),\n    keras.layers.Conv2D(64,(3,3),activation='relu',padding='same'),\n    keras.layers.MaxPool2D(),\n    keras.layers.GlobalAveragePooling2D(),\n    keras.layers.Dense(43, activation='softmax')\n], name=\"ultra_light_cnn\")\n\nmodel_slim.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### MobileNetV2","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras import layers, models\n\nIMG_HEIGHT_MOBILENETV2 = 32\nIMG_WIDTH_MOBILENETV2 = 32\n\ndef get_X_train_MobileNetV2():\n    image_data = []\n    image_labels = []\n    \n    for i in range(NUM_CATEGORIES):\n        path = data_dir + '/train/' + f\"{i:02d}\"\n    \n        # Skip folder of label that not exists \n        if not os.path.isdir(path):\n            continue\n        images = os.listdir(path)\n    \n        for img in images:\n            try:\n                image = cv2.imread(path + '/' + img)\n                image_fromarray = Image.fromarray(image, 'RGB')\n                resize_image = image_fromarray.resize((IMG_HEIGHT_MOBILENETV2, IMG_WIDTH_MOBILENETV2))\n                image_data.append(np.array(resize_image))\n                image_labels.append(f\"{i:02d}\")\n            except:\n                print(\"Error in \" + img)\n    \n    # Changing the list to numpy array\n    image_data = np.array(image_data)\n    image_labels = np.array(image_labels)\n    return image_data.astype('float32') / 255. , image_labels\n\ndef get_X_val_MobileNetV2():\n    val_data = []\n    val_labels = []\n    for label_name in os.listdir(val_set):\n        label_dir = os.path.join(val_set, label_name)\n        if not os.path.isdir(label_dir):\n            continue\n        for img_file in os.listdir(label_dir):\n            img_path = os.path.join(label_dir, img_file)\n            img = cv2.imread(img_path)\n            img = Image.fromarray(img, 'RGB').resize((IMG_HEIGHT_MOBILENETV2, IMG_WIDTH_MOBILENETV2))\n            val_data.append(np.array(img))\n            val_labels.append(int(label_name))\n    \n    X_val = np.array(val_data, dtype='float32') / 255.\n    return X_val, val_labels\n\nX_train_MobileNetV2, image_labels_MobileNetV2 = get_X_train_MobileNetV2()\nX_val_MobileNetV2, val_labels_MobileNetV2 = get_X_val_MobileNetV2()\n\ny_train_MobileNetV2 = keras.utils.to_categorical(image_labels_MobileNetV2, NUM_CATEGORIES)\nval_labels_MobileNetV2 = np.array(val_labels) \ny_val_MobileNetV2 = keras.utils.to_categorical(val_labels_MobileNetV2, NUM_CATEGORIES)\n\nbase = MobileNetV2(input_shape=(IMG_HEIGHT_MOBILENETV2,IMG_WIDTH_MOBILENETV2,channels), include_top=False, weights=None)\nx = base.output\nx = layers.GlobalAveragePooling2D()(x)\noutputs = layers.Dense(43, activation='softmax')(x)\nmodel_mnv2 = models.Model(inputs=base.input, outputs=outputs, name=\"mobilenet_v2\")\n\nmodel_mnv2.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Augmenting the data and training the model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nfrom keras.callbacks import LearningRateScheduler \nfrom keras.callbacks import ModelCheckpoint\n\n# Data augmentation\naug = ImageDataGenerator(\n    rotation_range=10,\n    zoom_range=0.15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.15,\n    horizontal_flip=False,\n    vertical_flip=False,\n    fill_mode=\"nearest\"\n)\naugmented_train = aug.flow(X_train, y_train, batch_size=32)\n\n# Monitor learning by validation accuracy\nearly_stop = EarlyStopping(\n    monitor='val_accuracy',\n    patience=10,                \n    restore_best_weights=True,\n    verbose=1                 \n)\n\ncheckpoint = ModelCheckpoint('model.weights.h5', save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min')\n\ndef lr_decay(epoch):\n    return lr * (0.5 ** (epoch // (epochs * 0.5)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if os.path.isfile('/kaggle/input/weights/model.weights.h5'):\n    model.load_weights('/kaggle/input/weights/model.weights.h5')\nelse:\n    opt = Adam(learning_rate=lr)\n    model.compile(\n        loss='categorical_crossentropy',\n        optimizer=opt,\n        metrics=['accuracy']\n    )\n    \n    history_model = model.fit(\n        augmented_train,\n        epochs=epochs,\n        validation_data=(X_val, y_val),\n        callbacks=[early_stop, LearningRateScheduler(lr_decay), checkpoint]\n    )\n    model.save_weights('model.weights.h5')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if os.path.isfile('/kaggle/input/weights/model_imp.weights.h5'):\n    model.load_weights('/kaggle/input/weights/model_imp.weights.h5')\nelse:\n    opt = Adam(learning_rate=lr)\n    model_imp.compile(\n        loss='categorical_crossentropy',\n        optimizer=opt,\n        metrics=['accuracy']\n    )\n\n    history_model_imp = model_imp.fit(\n        augmented_train,\n        epochs=50,\n        validation_data=(X_val, y_val),\n        callbacks=[early_stop, LearningRateScheduler(lr_decay), checkpoint]\n    )\n    model_imp.save_weights('model_imp.weights.h5')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if os.path.isfile('/kaggle/input/weights/model_slim.weights.h5'):\n    model.load_weights('/kaggle/input/weights/model_slim.weights.h5')\nelse:\n    opt = Adam(learning_rate=lr)\n    model_slim.compile(\n        loss='categorical_crossentropy',\n        optimizer=opt,\n        metrics=['accuracy']\n    )\n    \n    history_model_slim = model_slim.fit(\n        augmented_train,\n        epochs=50,\n        validation_data=(X_val, y_val),\n        callbacks=[early_stop,LearningRateScheduler(lr_decay), checkpoint]\n    )\n    model_slim.save_weights('model_slim.weights.h5')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"opt = Adam(learning_rate=lr)\n\nmodel_mnv2.compile(\n    loss='categorical_crossentropy',\n    optimizer=opt,\n    metrics=['accuracy']\n)\n\n# Training con EarlyStopping\nhistory_model_mnv2 = model_mnv2.fit(\n    aug.flow(X_train_MobileNetV2, y_train_MobileNetV2, batch_size=32),\n    epochs=30,\n    validation_data=(X_val_MobileNetV2, y_val_MobileNetV2),\n    callbacks=[LearningRateScheduler(lr_decay), checkpoint]\n)\nmodel_mnv2.save_weights('model_mnv2.weights.h5')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 4, figsize=(24, 5))\n\n#histories = [history_model, history_model_imp, history_model_slim, history_model_mnv2]\nhistories = [history_model, history_model_imp, history_model_slim]\n#models = [model, model_imp, model_slim, model_mnv2]\nmodels = [model, model_imp, model_slim]\n#titles = ['Training original model', 'Training improved model', 'Training slim model', 'MobileNetV2']\ntitles = ['Training original model', 'Training improved model', 'Training slim model']\n\nfor i, (hist, model, title) in enumerate(zip(histories, models, titles)):\n    pd.DataFrame(hist.history).plot(ax=axs[i])\n    axs[i].set_title(f\"{title}\\nParams: {model.count_params():,}\")\n    axs[i].grid(True)\n    axs[i].set_ylim(0, 1)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluating the model","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Quantization Optimitation ","metadata":{}},{"cell_type":"code","source":"converter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT] # default to 8bit\nquant_model = converter.convert()\n\n_, quant_model_file = tempfile.mkstemp('.tflite')\n\nwith open(quant_model_file, 'wb') as f:\n  f.write(quant_model)\n\nprint('Saved quant TFLite model to:', quant_model_file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_gzipped_model_size(file):\n  # Returns size of gzipped model, in bytes.\n  import os\n  import zipfile\n\n  _, zipped_file = tempfile.mkstemp('.zip')\n  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n    f.write(file)\n\n  return os.path.getsize(zipped_file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Size of gzipped baseline Keras model:\\t%.2f bytes\" % (get_gzipped_model_size(model_file)))\nprint(\"Size of gzipped quant Keras model:\\t%.2f bytes\" % (get_gzipped_model_size(quant_model_file)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the test data and running the predictions","metadata":{}},{"cell_type":"code","source":"import glob\n\nimgs = []\nlabels = []\n\nfor label_name in os.listdir(test_path):\n    label_dir = os.path.join(test_path, label_name)\n    if os.path.isdir(label_dir):\n        # Cerca immagini dentro la cartella della label\n        for img_path in glob.glob(os.path.join(label_dir, '*')):\n            imgs.append(img_path)\n            labels.append(label_name)\n\nlabels = np.array(labels).astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_predictions(model):\n    data = []\n\n    for img in imgs:\n        image = cv2.imread(img)\n        image_fromarray = Image.fromarray(image, 'RGB')\n        resize_image = image_fromarray.resize((IMG_HEIGHT, IMG_WIDTH))\n        data.append(np.array(resize_image))\n    X_test = np.array(data)\n    X_test = X_test/255\n    \n    pred = np.argmax(model.predict(X_test), axis=-1)\n    return pred, X_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Accuracy with the test data\nprint('Test Data accuracy original model: ',accuracy_score(labels, get_predictions(model)[0])*100)\nprint('Test Data accuracy improved model: ',accuracy_score(labels, get_predictions(model_imp)[0])*100)\nprint('Test Data accuracy improved model: ',accuracy_score(labels, get_predictions(model_slim)[0])*100)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Weight Clustering","metadata":{}},{"cell_type":"code","source":"cluster_weights = tfmot.clustering.keras.cluster_weights\nCentroidInitialization = tfmot.clustering.keras.CentroidInitialization\n\nclustering_params = {\n  'number_of_clusters': 16,\n  'cluster_centroids_init': CentroidInitialization.LINEAR\n}\n\n# Cluster a whole model\nclustered_model = cluster_weights(model_imp, **clustering_params)\n\n# Use smaller learning rate for fine-tuning clustered model\nopt = keras.optimizers.Adam(learning_rate=1e-5)\n\nclustered_model.compile(\n  loss='categorical_crossentropy',\n  optimizer=opt,\n  metrics=['accuracy'])\n\nclustered_model.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fine-tune model\nclustered_model.fit(\n  aug.flow(X_train, y_train, batch_size=500),\n  epochs=1,\n  validation_data=(X_val, y_val)\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing the confusion matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncf = confusion_matrix(labels,  get_predictions(model_imp))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\ndf_cm = pd.DataFrame(cf, index = classes,  columns = classes)\nplt.figure(figsize = (20,20))\nsns.heatmap(df_cm, annot=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Classification report","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(labels, get_predictions(model)[0]))\nprint(classification_report(labels, get_predictions(model_imp)[0]))\nprint(classification_report(labels, get_predictions(model_slim)[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Predictions on Test Data","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (25, 25))\n\npred, X_test = get_predictions(model_imp)\n\nstart_index = 0\nfor i in range(25):\n    plt.subplot(5, 5, i + 1)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    prediction = pred[start_index + i]\n    actual = labels[start_index + i]\n    col = 'g'\n    if prediction != actual:\n        col = 'r'\n    plt.xlabel('Actual={} || Pred={}'.format(actual, prediction), color = col)\n    plt.imshow(X_test[start_index + i])\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Lime Explainability Evaluation","metadata":{}},{"cell_type":"code","source":"def predict_fn_1(images_np):\n    images_np = images_np / 255.0\n    return model.predict(images_np)\n\ndef predict_fn_2(images_np):\n    images_np = images_np / 255.0\n    return model_imp.predict(images_np)\n\ndef predict_fn_3(images_np):\n    images_np = images_np / 255.0\n    return model_slim.predict(images_np)\n\nmodels = [\n    (\"Modello di base\", predict_fn_1),\n    (\"Modello ottimizzato\", predict_fn_2),\n    (\"Modello slim\", predict_fn_3)\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\nimport numpy as np\n\nimg_path = \"/kaggle/input/dataset-2025-04-23-eps-100-changed/dataset_20250423_200322_eps_100_changed/test/22/132836142802631_29.jpg\"\nimg = image.load_img(img_path)\nimg_np = image.img_to_array(img)\nimg_np = np.uint8(img_np)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from skimage.segmentation import mark_boundaries\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.ndimage import gaussian_filter\n\ndef show_lime_explanation(image, explainer, predict_fn, model_name):\n    \"\"\"\n    Genera una spiegazione LIME per un singolo modello.\n    Restituisce l'immagine processata, la heatmap e il nome del modello.\n    \"\"\"\n    explanation = explainer.explain_instance(image, predict_fn, top_labels=5, hide_color=0, num_samples=1000)\n    top_label = explanation.top_labels[0]\n    weights = dict(explanation.local_exp[top_label])\n    \n    temp, mask = explanation.get_image_and_mask(\n        label=top_label,\n        positive_only=False,\n        num_features=15,\n        hide_rest=False\n    )\n    \n    # Creiamo una heatmap pi√π dettagliata\n    heatmap = np.zeros_like(mask, dtype=float)\n    for seg_idx, weight in weights.items():\n        if abs(weight) > 0.005:  # Soglia pi√π bassa per catturare pi√π dettagli\n            heatmap[mask == seg_idx] = weight\n    \n    # Normalizzazione simmetrica per preservare segno e intensit√†\n    max_abs_weight = np.max(np.abs(heatmap))\n    if max_abs_weight > 0:\n        heatmap = heatmap / max_abs_weight\n    \n    # Applicazione di un filtro gaussiano per smoothing graduato\n    heatmap_smooth = gaussian_filter(heatmap, sigma=1.5)\n    \n    return temp, heatmap_smooth, model_name","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lime_results = []\nfor i, (name, predict_fn) in enumerate(models):\n    print(f\"Processando {name}... ({i+1}/3)\")\n    temp, heatmap, model_name = show_lime_explanation(img_np, explainer, predict_fn, name)\n    lime_results.append((temp, heatmap, model_name))\n\n# Estrazione delle heatmap per la normalizzazione comune\nheatmaps = [result[1] for result in lime_results]\n\n# Normalizziamo tutte le heatmap sulla stessa scala per confronto\nall_heatmaps = np.concatenate([h.flatten() for h in heatmaps])\nvmin, vmax = np.percentile(all_heatmaps, [5, 95])  # Usa percentili per evitare outlier\n\n# Visualizzazione migliorata\nfig, axes = plt.subplots(1, 4, figsize=(20, 6))\n\n# Immagine originale\naxes[0].imshow(img_np.astype(np.uint8))\naxes[0].set_title(\"Immagine originale\", fontsize=12, fontweight='bold')\naxes[0].axis('off')\n\n# Visualizzazione delle heatmap usando i risultati pre-calcolati\nfor i, (temp, heatmap, model_name) in enumerate(lime_results):\n    # Mostra l'immagine originale\n    axes[i + 1].imshow(temp.astype(np.uint8))\n    \n    # Overlay della heatmap con colormap migliorata\n    im = axes[i + 1].imshow(heatmap, \n                           cmap='RdYlBu_r',  # Colormap che va dal blu (negativo) al rosso (positivo)\n                           alpha=0.6,        # Trasparenza per vedere l'immagine sotto\n                           vmin=vmin, \n                           vmax=vmax,\n                           interpolation='bilinear')  # Interpolazione per smoothness\n    \n    axes[i + 1].set_title(f\"{model_name}\", fontsize=12, fontweight='bold')\n    axes[i + 1].axis('off')\n\n# Colorbar migliorata\nfig.subplots_adjust(right=0.88)\ncbar_ax = fig.add_axes([0.90, 0.15, 0.02, 0.7])\ncbar = fig.colorbar(im, cax=cbar_ax)\ncbar.set_label(\"Importanza\\n(Rosso=Positiva, Blu=Negativa)\", fontsize=10, fontweight='bold')\ncbar.ax.tick_params(labelsize=9)\n\n# Layout finale\nplt.tight_layout()\nplt.subplots_adjust(right=0.88)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Analisi testuali risultati spiegabilit√†","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"ANALISI DETTAGLIATA DELLE SPIEGAZIONI LIME\")\nprint(\"=\"*80)\n\ndef analyze_lime_result(heatmap, model_name):\n    \"\"\"\n    Analizza una heatmap LIME e restituisce statistiche descrittive.\n    \"\"\"\n    # Calcolo statistiche di base\n    positive_weights = heatmap[heatmap > 0]\n    negative_weights = heatmap[heatmap < 0]\n    \n    total_pixels = heatmap.size\n    positive_pixels = len(positive_weights)\n    negative_pixels = len(negative_weights)\n    neutral_pixels = total_pixels - positive_pixels - negative_pixels\n    \n    # Percentuali\n    pos_percentage = (positive_pixels / total_pixels) * 100\n    neg_percentage = (negative_pixels / total_pixels) * 100\n    neutral_percentage = (neutral_pixels / total_pixels) * 100\n    \n    # Statistiche sui pesi\n    max_positive = np.max(positive_weights) if len(positive_weights) > 0 else 0\n    max_negative = np.min(negative_weights) if len(negative_weights) > 0 else 0\n    mean_positive = np.mean(positive_weights) if len(positive_weights) > 0 else 0\n    mean_negative = np.mean(negative_weights) if len(negative_weights) > 0 else 0\n    \n    # Intensit√† complessiva\n    total_positive_influence = np.sum(positive_weights) if len(positive_weights) > 0 else 0\n    total_negative_influence = np.sum(np.abs(negative_weights)) if len(negative_weights) > 0 else 0\n    \n    return {\n        'positive_pixels': positive_pixels,\n        'negative_pixels': negative_pixels,\n        'neutral_pixels': neutral_pixels,\n        'pos_percentage': pos_percentage,\n        'neg_percentage': neg_percentage,\n        'neutral_percentage': neutral_percentage,\n        'max_positive': max_positive,\n        'max_negative': max_negative,\n        'mean_positive': mean_positive,\n        'mean_negative': mean_negative,\n        'total_positive_influence': total_positive_influence,\n        'total_negative_influence': total_negative_influence\n    }\n\ndef interpret_lime_behavior(stats, model_name):\n    \"\"\"\n    Interpreta le statistiche LIME e fornisce una descrizione comportamentale.\n    \"\"\"\n    interpretation = []\n    \n    # Analisi della distribuzione delle influenze\n    if stats['pos_percentage'] > stats['neg_percentage']:\n        if stats['pos_percentage'] > 60:\n            interpretation.append(\"Il modello si basa prevalentemente su caratteristiche positive dell'immagine per la sua predizione.\")\n        else:\n            interpretation.append(\"Il modello utilizza principalmente caratteristiche positive, ma considera anche alcuni aspetti negativi.\")\n    elif stats['neg_percentage'] > stats['pos_percentage']:\n        if stats['neg_percentage'] > 60:\n            interpretation.append(\"Il modello sembra essere principalmente influenzato da caratteristiche che considera controindizi.\")\n        else:\n            interpretation.append(\"Il modello bilancia caratteristiche positive e negative, con una leggera prevalenza dei controindizi.\")\n    else:\n        interpretation.append(\"Il modello presenta un bilanciamento equilibrato tra caratteristiche positive e negative.\")\n    \n    # Analisi dell'intensit√†\n    if stats['max_positive'] > 0.8:\n        interpretation.append(\"Presenza di aree ad alta confidenza positiva, indicando caratteristiche molto distintive per la classe predetta.\")\n    elif stats['max_positive'] > 0.5:\n        interpretation.append(\"Il modello identifica alcune caratteristiche moderatamente importanti per la predizione.\")\n    else:\n        interpretation.append(\"Il modello mostra una confidenza relativamente bassa nelle caratteristiche individuate.\")\n    \n    if abs(stats['max_negative']) > 0.8:\n        interpretation.append(\"Presenza di forti controindizi, suggerendo caratteristiche che il modello associa ad altre classi.\")\n    elif abs(stats['max_negative']) > 0.5:\n        interpretation.append(\"Il modello identifica alcuni controindizi moderati nella predizione.\")\n    \n    # Analisi della selettivit√†\n    if stats['neutral_percentage'] > 70:\n        interpretation.append(\"Il modello √® molto selettivo, concentrandosi solo su specifiche aree dell'immagine.\")\n    elif stats['neutral_percentage'] > 50:\n        interpretation.append(\"Il modello mostra una selettivit√† moderata nell'analisi dell'immagine.\")\n    else:\n        interpretation.append(\"Il modello considera una vasta porzione dell'immagine nella sua decisione.\")\n    \n    # Analisi dell'influenza totale\n    total_influence = stats['total_positive_influence'] + stats['total_negative_influence']\n    if total_influence > stats['positive_pixels'] * 0.6:  # Soglia arbitraria\n        interpretation.append(\"Il modello mostra una forte reattivit√† complessiva alle caratteristiche dell'immagine.\")\n    else:\n        interpretation.append(\"Il modello presenta una reattivit√† moderata alle caratteristiche visive.\")\n    \n    return interpretation\n\n# Analisi per ogni modello\nfor i, (temp, heatmap, model_name) in enumerate(lime_results):\n    print(f\"\\nüîç MODELLO: {model_name.upper()}\")\n    print(\"-\" * 50)\n    \n    # Calcolo statistiche\n    stats = analyze_lime_result(heatmap, model_name)\n    \n    # Stampa statistiche numeriche\n    print(f\"üìä STATISTICHE NUMERICHE:\")\n    print(f\"   ‚Ä¢ Pixel con influenza positiva: {stats['positive_pixels']:,} ({stats['pos_percentage']:.1f}%)\")\n    print(f\"   ‚Ä¢ Pixel con influenza negativa: {stats['negative_pixels']:,} ({stats['neg_percentage']:.1f}%)\")\n    print(f\"   ‚Ä¢ Pixel neutrali: {stats['neutral_pixels']:,} ({stats['neutral_percentage']:.1f}%)\")\n    print(f\"   ‚Ä¢ Massima influenza positiva: {stats['max_positive']:.3f}\")\n    print(f\"   ‚Ä¢ Massima influenza negativa: {stats['max_negative']:.3f}\")\n    print(f\"   ‚Ä¢ Influenza positiva media: {stats['mean_positive']:.3f}\")\n    print(f\"   ‚Ä¢ Influenza negativa media: {stats['mean_negative']:.3f}\")\n    \n    # Interpretazione comportamentale\n    interpretations = interpret_lime_behavior(stats, model_name)\n    print(f\"\\nüß† INTERPRETAZIONE COMPORTAMENTALE:\")\n    for j, interp in enumerate(interpretations, 1):\n        print(f\"   {j}. {interp}\")\n    \n    # Confronto relativo (solo dal secondo modello in poi)\n    if i > 0:\n        prev_stats = analyze_lime_result(lime_results[i-1][1], lime_results[i-1][2])\n        print(f\"\\nüîÑ CONFRONTO CON {lime_results[i-1][2].upper()}:\")\n        \n        pos_diff = stats['pos_percentage'] - prev_stats['pos_percentage']\n        if abs(pos_diff) > 5:  # Differenza significativa\n            direction = \"pi√π\" if pos_diff > 0 else \"meno\"\n            print(f\"   ‚Ä¢ Questo modello √® {direction} orientato verso caratteristiche positive ({pos_diff:+.1f}%)\")\n        \n        max_pos_diff = stats['max_positive'] - prev_stats['max_positive']\n        if abs(max_pos_diff) > 0.1:\n            direction = \"maggiore\" if max_pos_diff > 0 else \"minore\"\n            print(f\"   ‚Ä¢ Mostra una confidenza {direction} nelle caratteristiche chiave ({max_pos_diff:+.3f})\")\n        \n        neutral_diff = stats['neutral_percentage'] - prev_stats['neutral_percentage']\n        if abs(neutral_diff) > 10:\n            direction = \"pi√π\" if neutral_diff > 0 else \"meno\"\n            print(f\"   ‚Ä¢ √à {direction} selettivo nell'analisi dell'immagine ({neutral_diff:+.1f}%)\")\n\nprint(f\"\\n\" + \"=\"*80)\nprint(\"RIEPILOGO COMPARATIVO\")\nprint(\"=\"*80)\n\n# Riepilogo finale comparativo\nbest_positive = max(lime_results, key=lambda x: analyze_lime_result(x[1], x[2])['max_positive'])\nmost_selective = max(lime_results, key=lambda x: analyze_lime_result(x[1], x[2])['neutral_percentage'])\nmost_balanced = min(lime_results, key=lambda x: abs(analyze_lime_result(x[1], x[2])['pos_percentage'] - analyze_lime_result(x[1], x[2])['neg_percentage']))\n\nprint(f\"üèÜ Modello con maggiore confidenza: {best_positive[2]}\")\nprint(f\"üéØ Modello pi√π selettivo: {most_selective[2]}\")\nprint(f\"‚öñÔ∏è  Modello pi√π bilanciato: {most_balanced[2]}\")\nprint(\"=\"*80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Studio disomogeneit√† geografiche","metadata":{}},{"cell_type":"code","source":"def get_predictions_for_area_study(model, image_paths, img_size=(IMG_HEIGHT, IMG_WIDTH)):\n    data = []\n\n    for img_path in image_paths:\n        try:\n            img = cv2.imread(img_path)  # BGR\n            img_pil = Image.fromarray(img, mode='RGB')\n            img_resized = img_pil.resize(img_size)\n            data.append(np.array(img_resized))\n        except Exception as e:\n            print(f\"Errore con immagine {img_path}: {e}\")\n            data.append(np.zeros((*img_size, 3)))  # Fallback\n\n    X = np.array(data, dtype='float32') / 255.0\n    preds = np.argmax(model.predict(X), axis=-1)\n    \n    return preds\n\ndef geo_label(lat):\n    if lat < 41.5594700:\n        return 'sud'\n    elif lat > 44.801485:\n        return 'nord'\n    else:\n        return 'centre'\n\ndef build_paths(df, base_path='./'):\n    return [\n        os.path.join(base_path, 'test', f\"{row['feature_index']:02d}\", row['filename'])\n        for _, row in df.iterrows()\n    ]\n\nmodel_dict = {\n    'Model': model,\n    'Model_Imp': model_imp,\n    'Model_Slim': model_slim\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sign_info = pd.read_parquet(f'{data_dir}/metadata.parquet')\nsign_info = sign_info.loc[sign_info['split'] == 'test']\n\n# Da rimuovere\nsign_info[['lat', 'lon']] = sign_info['coordinates'].str.split(\",\", expand=True).astype(float)\nsign_info['geo_area'] = sign_info['lat'].apply(geo_label)\n\nsign_info_nord = sign_info[sign_info['geo_area'] == 'nord']\nsign_info_centre = sign_info[sign_info['geo_area'] == 'centre']\nsign_info_sud = sign_info[sign_info['geo_area'] == 'sud']\n\narea_dfs = {'nord': sign_info_nord, 'centre': sign_info_centre, 'sud': sign_info_sud}\naccuracy_results = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for model_name, mdl in model_dict.items():\n    for area_name, df_area in area_dfs.items():\n        image_paths = build_paths(df_area, data_dir)\n        preds = get_predictions_for_area_study(mdl, image_paths)\n        true_labels = df_area['feature_index'].values\n        acc = np.mean(preds == true_labels)\n        accuracy_results.append({\n            'modello': model_name,\n            'area': area_name,\n            'accuracy': acc\n        })\n\nresults_df = pd.DataFrame(accuracy_results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\n\nplt.figure(figsize=(8, 5))\nheatmap_data = results_df.pivot(index='modello', columns='area', values='accuracy')\n\nsns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', fmt=\".2f\", cbar=True)\nplt.title(\"Accuracy per area geografica per modello\")\nplt.xlabel(\"Area\")\nplt.ylabel(\"Modello\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}